一、wordcloud简介
词云，又称文字云、标签云，是对文本数据中出现频率较高的“关键词”在视觉上的突出呈现，形成关键词的渲染形成类似云一样的彩色图片，从而一眼就可以领略文本数据的主要表达意思。创建于文本分析及其可视化中。

除了网上现成的Wordle、Tagxedo、Tagul、Tagcrowd等词云制作工具，在python中也可以用wordcloud包比较轻松地实现。

官网：https://pypi.org/project/wordcloud/

github项目：https://github.com/amueller/word_cloud

二、wordcloud原理
同wordcloud做词云，主要做的是三件事：

文本预处理
词频统计
将高频词以图片形式进行彩色渲染
根据源码来看，wordcloud实现词云的步骤如下：

wordcloud制作词云时，首先要对对文本数据进行分词，使用process_text（）方法，这一步的主要任务是去除停用词
第二步是计算每个词在文本中出现的频率，生成一个哈希表。词频用于确定一个词的重要性
根据词频的数值按比例生成一个图片的布局，类IntegralOccupancyMap 是该词云的算法所在，是词云的数据可视化方式的核心。生成词的颜色、位置、方向等
最后将词按对应的词频在词云布局图上生成图片，核心方法是generate_from_frequencies,不论是generate（）还是generate_from_text（）都最终用到generate_from_frequencies 完成词云上各词的着色,默认是随机着色
词语的各种增强功能大都可以通过wordcloud的构造函数实现，里面提供了22个参数，还可以自行扩展。
三、wordcloud参数
参数	解释
font_path : string	字体路径，需要展现什么字体就把该字体路径+后缀名写上，如：font_path = '黑体.ttf'
width : int (default=400)	输出的画布宽度，默认为400像素
height : int (default=200)	输出的画布高度，默认为200像素
prefer_horizontal : float (default=0.90)	词语水平方向排版出现的频率，默认 0.9 （所以词语垂直方向排版出现频率为 0.1 ）
mask : nd-array or None (default=None)	如果参数为空，则使用二维遮罩绘制词云。如果 mask 非空，设置的宽高值将被忽略，遮罩形状被 mask 取代。除全白（#FFFFFF）的部分将不会绘制，其余部分会用于绘制词云。如：bg_pic = imread('读取一张图片.png')，背景图片的画布一定要设置为白色（#FFFFFF），然后显示的形状为不是白色的其他颜色。可以用ps工具将自己要显示的形状复制到一个纯白色的画布上再保存，就ok了。
scale : float (default=1)	按照比例进行放大画布，如设置为1.5，则长和宽都是原来画布的1.5倍。
min_font_size : int (default=4)	显示的最小的字体大小
font_step : int (default=1)	字体步长，如果步长大于1，会加快运算但是可能导致结果出现较大的误差。
max_words : number (default=200)	要显示的词的最大个数
stopwords : set of strings or None	设置需要屏蔽的词，如果为空，则使用内置的STOPWORDS
background_color : color value (default=”black”)	背景颜色，如
max_font_size : int or None (default=None)	显示的最大的字体大小
mode : string (default=”RGB”)	当参数为“RGBA”并且background_color不为空时，背景为透明。
relative_scaling : float (default=.5)	词频和字体大小的关联性
color_func : callable, default=None	生成新颜色的函数，如果为空，则使用 self.color_func
regexp : string or None (optional)	使用正则表达式分隔输入的文本
collocations : bool, default=True	是否包括两个词的搭配
colormap : string or matplotlib colormap, default=”viridis”	给每个单词随机分配颜色，若指定color_func，则忽略该方法。
fit_words(frequencies)	根据词频生成词云
generate(text)	根据文本生成词云
generate_from_frequencies(frequencies[, ...])	根据词频生成词云
generate_from_text(text)	根据文本生成词云
process_text(text)	将长文本分词并去除屏蔽词（此处指英语，中文分词还是需要自己用别的库先行实现，使用上面的 fit_words(frequencies) ）
recolor([random_state, color_func, colormap])	对现有输出重新着色。重新上色会比重新生成整个词云快很多。to_array() //转化为 numpy array
四、代码示例
import matplotlib.pyplot as plt
from wordcloud import WordCloud, ImageColorGenerator, STOPWORDS
import jieba
import numpy as np
from PIL import Image
def wordyun():
    #导入背景图   
    backgrim=np.array(Image.open(r"C:\Users\Administrator\Desktop\椭圆.png"))
    #导入文本文件   
    text=open(r"C:\Users\Administrator\Desktop\斗.破苍穹.txt",encoding='gbk').read()

    #jieba分词
    wordlist = jieba.cut(text, cut_all=True)
    wl = " ".join(wordlist)
#设置参数
    wordcloud=WordCloud(
        background_color='white',  #背景颜色
        mask=backgrim ,   #背景图片
        max_words = 1000,  # 设置最多现实的词数
        stopwords=STOPWORDS,  # 设置停用词
        max_font_size=100,    # 设置字体最大值
        font_path='C:/Windows/Fonts/simfang.ttf',#设置字体，路径在电脑内
        width=2000,
        height=1500,
        random_state=30,  # 设置有多少种随机生成状态，即有多少种配色方案
        # scale=.5
    ).generate(text)
#改变字体颜色
    image_colors = ImageColorGenerator(backgrim)
#展示词云
    plt.imshow(wordcloud)
#是否显示想x，y坐标
    plt.axis("off")
    plt.show()
#写入文件
    wordcloud.to_file('py_book1.png')  # 把词云保存下

wordyun()
